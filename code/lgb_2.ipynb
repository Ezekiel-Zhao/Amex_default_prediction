{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329e09a5",
   "metadata": {
    "papermill": {
     "duration": 0.003138,
     "end_time": "2022-08-07T15:12:24.656835",
     "exception": false,
     "start_time": "2022-08-07T15:12:24.653697",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e88bd2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-07T15:12:24.666759Z",
     "iopub.status.busy": "2022-08-07T15:12:24.666071Z",
     "iopub.status.idle": "2022-08-07T15:12:26.872621Z",
     "shell.execute_reply": "2022-08-07T15:12:26.871204Z"
    },
    "papermill": {
     "duration": 2.215568,
     "end_time": "2022-08-07T15:12:26.875938",
     "exception": false,
     "start_time": "2022-08-07T15:12:24.660370",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training feature engineer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 345/345 [00:24<00:00, 13.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 125.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 458913/458913 [03:10<00:00, 2409.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test feature engineer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 345/345 [00:49<00:00,  6.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 62.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 924621/924621 [06:28<00:00, 2381.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from itertools import combinations\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "def get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "def read_preprocess_data():\n",
    "    train = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/train.parquet')\n",
    "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\",\n",
    "    ]\n",
    "    num_features = [col for col in features if col not in cat_features]\n",
    "    print('Starting training feature engineer...')\n",
    "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "    train_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in train_num_agg:\n",
    "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
    "            if 'last' in col and col.replace('last', col_2) in train_num_agg:\n",
    "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', col_2)]\n",
    "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', col_2)]\n",
    "\n",
    "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "    train_cat_agg.reset_index(inplace = True)\n",
    "    train_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv')\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    train_diff = get_difference(train, num_features)\n",
    "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
    "    del train_num_agg, train_cat_agg, train_diff\n",
    "    gc.collect()\n",
    "    test = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/test.parquet')\n",
    "    print('Starting test feature engineer...')\n",
    "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "    test_num_agg.reset_index(inplace = True)\n",
    "\n",
    "    # Lag Features\n",
    "    for col in test_num_agg:\n",
    "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
    "            if 'last' in col and col.replace('last', col_2) in test_num_agg:\n",
    "                test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', col_2)]\n",
    "                test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', col_2)]\n",
    "\n",
    "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "    test_cat_agg.reset_index(inplace = True)\n",
    "    # Transform float64 columns to float32\n",
    "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
    "    # Transform int64 columns to int32\n",
    "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
    "    for col in tqdm(cols):\n",
    "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
    "    # Get the difference\n",
    "    test_diff = get_difference(test, num_features)\n",
    "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
    "    del test_num_agg, test_cat_agg, test_diff\n",
    "    gc.collect()\n",
    "    # Save files to disk\n",
    "    train.to_parquet('train_fe_v3_loaded.parquet')\n",
    "    test.to_parquet('test_fe_v3_loaded.parquet')\n",
    "    \n",
    "# Read & Preprocess Data\n",
    "read_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059190bb",
   "metadata": {
    "papermill": {
     "duration": 0.003419,
     "end_time": "2022-08-07T15:12:26.883207",
     "exception": false,
     "start_time": "2022-08-07T15:12:26.879788",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f034ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T15:12:26.892864Z",
     "iopub.status.busy": "2022-08-07T15:12:26.892432Z",
     "iopub.status.idle": "2022-08-07T15:12:26.909378Z",
     "shell.execute_reply": "2022-08-07T15:12:26.908418Z"
    },
    "papermill": {
     "duration": 0.024494,
     "end_time": "2022-08-07T15:12:26.911755",
     "exception": false,
     "start_time": "2022-08-07T15:12:26.887261",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    target = 'target'\n",
    "    input_dir = './'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_parquet(CFG.input_dir + 'train_fe_v3_loaded.parquet')\n",
    "    test = pd.read_parquet(CFG.input_dir + 'test_fe_v3_loaded.parquet')\n",
    "    return train, test\n",
    "\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def amex_metric_np(preds, target):\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca19a9",
   "metadata": {
    "papermill": {
     "duration": 0.003256,
     "end_time": "2022-08-07T15:12:26.918713",
     "exception": false,
     "start_time": "2022-08-07T15:12:26.915457",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Training LightGBM (DART) Model\n",
    "\n",
    "- Final predictions output uploaded as a public dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27bf528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T15:12:26.928464Z",
     "iopub.status.busy": "2022-08-07T15:12:26.927689Z",
     "iopub.status.idle": "2022-08-07T15:12:26.951845Z",
     "shell.execute_reply": "2022-08-07T15:12:26.950257Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.032624,
     "end_time": "2022-08-07T15:12:26.954809",
     "exception": false,
     "start_time": "2022-08-07T15:12:26.922185",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1713/1713 [12:32<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2177 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.533444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 321842\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.336216\ttraining's amex_metric: 0.778536\tvalid_1's binary_logloss: 0.338248\tvalid_1's amex_metric: 0.772012\n",
      "[1000]\ttraining's binary_logloss: 0.245309\ttraining's amex_metric: 0.796131\tvalid_1's binary_logloss: 0.250986\tvalid_1's amex_metric: 0.782915\n",
      "[1500]\ttraining's binary_logloss: 0.221238\ttraining's amex_metric: 0.809444\tvalid_1's binary_logloss: 0.230889\tvalid_1's amex_metric: 0.789887\n",
      "[2000]\ttraining's binary_logloss: 0.207443\ttraining's amex_metric: 0.822108\tvalid_1's binary_logloss: 0.222253\tvalid_1's amex_metric: 0.793443\n",
      "[2500]\ttraining's binary_logloss: 0.200283\ttraining's amex_metric: 0.832887\tvalid_1's binary_logloss: 0.219423\tvalid_1's amex_metric: 0.797024\n",
      "[3000]\ttraining's binary_logloss: 0.193146\ttraining's amex_metric: 0.842875\tvalid_1's binary_logloss: 0.217191\tvalid_1's amex_metric: 0.798057\n",
      "[3500]\ttraining's binary_logloss: 0.186575\ttraining's amex_metric: 0.853409\tvalid_1's binary_logloss: 0.215578\tvalid_1's amex_metric: 0.799835\n",
      "[4000]\ttraining's binary_logloss: 0.180769\ttraining's amex_metric: 0.863591\tvalid_1's binary_logloss: 0.214678\tvalid_1's amex_metric: 0.801241\n",
      "[4500]\ttraining's binary_logloss: 0.175103\ttraining's amex_metric: 0.873161\tvalid_1's binary_logloss: 0.213978\tvalid_1's amex_metric: 0.80085\n",
      "[5000]\ttraining's binary_logloss: 0.169507\ttraining's amex_metric: 0.882636\tvalid_1's binary_logloss: 0.213394\tvalid_1's amex_metric: 0.801794\n",
      "[5500]\ttraining's binary_logloss: 0.164486\ttraining's amex_metric: 0.891379\tvalid_1's binary_logloss: 0.213075\tvalid_1's amex_metric: 0.801893\n",
      "[6000]\ttraining's binary_logloss: 0.160225\ttraining's amex_metric: 0.898757\tvalid_1's binary_logloss: 0.212838\tvalid_1's amex_metric: 0.80161\n",
      "[6500]\ttraining's binary_logloss: 0.155729\ttraining's amex_metric: 0.906138\tvalid_1's binary_logloss: 0.212636\tvalid_1's amex_metric: 0.801371\n",
      "[7000]\ttraining's binary_logloss: 0.150538\ttraining's amex_metric: 0.914798\tvalid_1's binary_logloss: 0.212379\tvalid_1's amex_metric: 0.801828\n",
      "[7500]\ttraining's binary_logloss: 0.145627\ttraining's amex_metric: 0.922532\tvalid_1's binary_logloss: 0.212182\tvalid_1's amex_metric: 0.80209\n",
      "[8000]\ttraining's binary_logloss: 0.141278\ttraining's amex_metric: 0.929413\tvalid_1's binary_logloss: 0.212067\tvalid_1's amex_metric: 0.803046\n",
      "[8500]\ttraining's binary_logloss: 0.137513\ttraining's amex_metric: 0.935838\tvalid_1's binary_logloss: 0.211936\tvalid_1's amex_metric: 0.803236\n",
      "[9000]\ttraining's binary_logloss: 0.133291\ttraining's amex_metric: 0.942028\tvalid_1's binary_logloss: 0.211837\tvalid_1's amex_metric: 0.803139\n",
      "[9500]\ttraining's binary_logloss: 0.129522\ttraining's amex_metric: 0.948062\tvalid_1's binary_logloss: 0.211751\tvalid_1's amex_metric: 0.803227\n",
      "[10000]\ttraining's binary_logloss: 0.12582\ttraining's amex_metric: 0.95303\tvalid_1's binary_logloss: 0.211729\tvalid_1's amex_metric: 0.803255\n",
      "[10500]\ttraining's binary_logloss: 0.122573\ttraining's amex_metric: 0.9577\tvalid_1's binary_logloss: 0.211734\tvalid_1's amex_metric: 0.803278\n",
      "Our fold 0 CV score is 0.8032776731168763\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2177 features...\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.380381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 322018\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.335914\ttraining's amex_metric: 0.780739\tvalid_1's binary_logloss: 0.339361\tvalid_1's amex_metric: 0.763807\n",
      "[1000]\ttraining's binary_logloss: 0.244689\ttraining's amex_metric: 0.797519\tvalid_1's binary_logloss: 0.252609\tvalid_1's amex_metric: 0.775283\n",
      "[1500]\ttraining's binary_logloss: 0.220418\ttraining's amex_metric: 0.811523\tvalid_1's binary_logloss: 0.233084\tvalid_1's amex_metric: 0.78226\n",
      "[2000]\ttraining's binary_logloss: 0.20659\ttraining's amex_metric: 0.824419\tvalid_1's binary_logloss: 0.224706\tvalid_1's amex_metric: 0.785889\n",
      "[2500]\ttraining's binary_logloss: 0.199433\ttraining's amex_metric: 0.834957\tvalid_1's binary_logloss: 0.221954\tvalid_1's amex_metric: 0.789426\n",
      "[3000]\ttraining's binary_logloss: 0.192323\ttraining's amex_metric: 0.844695\tvalid_1's binary_logloss: 0.219853\tvalid_1's amex_metric: 0.790058\n",
      "[3500]\ttraining's binary_logloss: 0.185724\ttraining's amex_metric: 0.85518\tvalid_1's binary_logloss: 0.218483\tvalid_1's amex_metric: 0.791267\n",
      "[4000]\ttraining's binary_logloss: 0.179919\ttraining's amex_metric: 0.865107\tvalid_1's binary_logloss: 0.217633\tvalid_1's amex_metric: 0.792511\n",
      "[4500]\ttraining's binary_logloss: 0.174264\ttraining's amex_metric: 0.874416\tvalid_1's binary_logloss: 0.217039\tvalid_1's amex_metric: 0.792855\n",
      "[5000]\ttraining's binary_logloss: 0.16867\ttraining's amex_metric: 0.884083\tvalid_1's binary_logloss: 0.216582\tvalid_1's amex_metric: 0.793721\n",
      "[5500]\ttraining's binary_logloss: 0.163659\ttraining's amex_metric: 0.892836\tvalid_1's binary_logloss: 0.216249\tvalid_1's amex_metric: 0.79306\n",
      "[6000]\ttraining's binary_logloss: 0.159413\ttraining's amex_metric: 0.900766\tvalid_1's binary_logloss: 0.216097\tvalid_1's amex_metric: 0.793341\n"
     ]
    }
   ],
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Label encode categorical features\n",
    "    cat_features = [\n",
    "        \"B_30\",\n",
    "        \"B_38\",\n",
    "        \"D_114\",\n",
    "        \"D_116\",\n",
    "        \"D_117\",\n",
    "        \"D_120\",\n",
    "        \"D_126\",\n",
    "        \"D_63\",\n",
    "        \"D_64\",\n",
    "        \"D_66\",\n",
    "        \"D_68\"\n",
    "    ]\n",
    "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        test[col + '_round2'] = test[col].round(2)\n",
    "    # Get the difference between last and mean\n",
    "    num_cols = [col for col in train.columns if 'last' in col]\n",
    "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
    "    for col in num_cols:\n",
    "        try:\n",
    "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
    "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
    "        except:\n",
    "            pass\n",
    "    # Transform float64 and float32 to float16\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    for col in tqdm(num_cols):\n",
    "        train[col] = train[col].astype(np.float16)\n",
    "        test[col] = test[col].astype(np.float16)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', CFG.target]]\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': CFG.seed,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 2,\n",
    "        'min_data_in_leaf': 40\n",
    "        }\n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[CFG.target].iloc[trn_ind], train[CFG.target].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 500,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, f'lgbm_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        # Predict validation\n",
    "        val_pred = model.predict(x_val)\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[val_ind] = val_pred\n",
    "        # Predict the test set\n",
    "        test_pred = model.predict(test[features])\n",
    "        test_predictions += test_pred / CFG.n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    # Compute out of folds metric\n",
    "    score = amex_metric(train[CFG.target], oof_predictions)\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(f'oof_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}_v2.csv', index = False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "    test_df.to_csv(f'test_lgbm_baseline_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)\n",
    "    test_df.to_csv(f'../sub/test_lgbm_5fold_seed42_v2.csv') \n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "train, test = read_data()\n",
    "train_and_evaluate(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c6255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca62dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccf7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.379671,
   "end_time": "2022-08-07T15:12:33.073782",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-07T15:12:13.694111",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
